---
title: "macroeconomic_forecasting_pca"
author: "Siqi Ge"
date: "2025-12-04"
output:
  pdf_document:
    latex_engine: xelatex
---


```{r}
# Load standard libraries
#install.packages("stats")
#install.packages("pracma")
#devtools::install_github("cykbennie/fbi", force = TRUE)

library(tidyverse)
library(ggplot2)
library(stargazer)
library(leaps) 

library(fbi) 
library(readr) 

#install.packages("tinytex")
#tinytex::install_tinytex()

```


```{r}
# Data Import & Initial Target Variable Preparation 

# Define the file path for the FRED-MD database.
filepath <- "C:/Users/asus/Desktop/2025-09-md.csv" 

# handles missing values, and applies the required stationary transformations.
X.raw <- fredmd(
  filepath, 
  date_start = as.Date("1961-01-01"), # Set the required start date for the analysis (1961.01)
  date_end = as.Date("2019-12-01"),   # Set the required end date (2019.12)
  transform = TRUE                    # 'transform = TRUE' applies the stationary 
                                      # transformations (e.g., log differences, first differences) 
                                      # as specified in the FRED-MD documentation.
)

# Prepare a working copy of the raw data. X.raw now contains all transformed series.
y.raw <- X.raw 

# Variable Information for Exclusion and Selection 
# Retrieve detailed descriptions and group information for all loaded variables.
var.info <- describe_md(names(X.raw), name.only = FALSE)
head(var.info)    # inspect the transformation and group columns

# Identify and Extract the Target Forecast Series (y) 
# The forecast target for this project is Real Manufacturing and Trade Sales (CMRMTSPLx).
all_y <- c("CMRMTSPLx")

# Extract the dates and set them as row names for time-series operations.
dates <- y.raw$date
rownames(y.raw) <- dates

# Extract transformed target series (used to construct h-step-ahead growth rates)
y <- y.raw[,"CMRMTSPLx"]

```

```{r}
# Construct Target Variable (CMRMTSPLx)

# Get the total number of observations in the time series (1961.01 to 2019.12)
obs <- length(y) 

# Initialize storage matrices for the h-step annualized average growth rates.
# The matrices are initialized with NA values and row names are set to the corresponding dates.
y6  <- matrix(NA, obs, 1); rownames(y6)  <- dates
y12 <- matrix(NA, obs, 1); rownames(y12) <- dates
y24 <- matrix(NA, obs, 1); rownames(y24) <- dates

# Calculate Annualized Average Growth Rates (Backward-Looking Implementation) 
# We need to calculate y_t^h = (1200/h) * sum_{i=0}^{h-1} log(X_{t-i}/X_{t-i-1}). Since the input 'y' is already the transformed monthly log difference (log(X_t/X_{t-1}) * 100), we simply sum the monthly changes and annualize the result by (1200/h).
for (i in seq(6, obs))  y6[i]  <- (1200/6)  * sum(y[(i-5):i])
for (i in seq(12, obs)) y12[i] <- (1200/12) * sum(y[(i-11):i])
for (i in seq(24, obs)) y24[i] <- (1200/24) * sum(y[(i-23):i])
```

```{r}
# Convert the raw data frame to a tibble 
X.clean <- as_tibble(X.raw, rownames = NA)

# Re-extract dates (although already done, this ensures the date variable is correctly managed).
dates <- X.clean$date

# Remove the date column from the working dataset, keeping only the series data.
X.clean <- X.clean %>% select(-date) 

# Set the dates as row names, which is often preferred for time-series operations and factor estimation.
rownames(X.clean) <- dates

# NA Filtering: Remove Variables with Excessive Missing Data 
# Calculate the proportion of missing values (NA) for each column (variable).
col_na_prop <- apply(is.na(X.clean), 2, mean)
col_na_prop[col_na_prop>0.01]   # Inspect variables with >1% missing values.

# Filter out variables where the proportion of NA values exceeds 1% (or any set threshold, here 0.01).
X.clean <- X.clean[, (col_na_prop < 0.01)]

# Exclusion of Project-Specific Group: Interest and Exchange Rates
# Display the unique groups available in the FRED-MD dataset for confirmation.
unique(var.info$group) 

# Identify the series names belonging to the group to be excluded
var.exclude <- var.info$fred[var.info$group == "Interest and Exchange Rates"]

# Find the intersection: Identify which variables in the exclusion list (var.exclude) are actually present in the cleaned data (names(X.clean)).
var.exclude <- intersect(names(X.clean), var.exclude)

# Use 'all_of()' to correctly select columns for exclusion using a character vector of names.
X <- X.clean %>% select(-all_of(var.exclude))
```

## Static Factor Model Framework

Assume that the vector of observed macroeconomic variables x_t follows a static factor model:
x_t = Lambda * f_t + e_t, where x_t is an N x 1 vector of observed macroeconomic variables; f_t is an r x 1 vector of unobserved common factors; Lambda is an N x r matrix of factor loadings; e_t is an N x 1 vector of idiosyncratic error terms.

The fundamental assumption of the model is parsimony: the number of factors r is assumed to be small relative to the number of variables N (r << N). This means that the systematic variation across the entire large dataset is driven by only a handful of underlying common forces.

The number of factors r is assumed to be small relative to N. The idiosyncratic errors may be weakly correlated across variables and over time, but their overall contribution is limited compared with the common components.

The common component (Lambda*f_t) captures the pervasive, systematic co-movement shared across all series, while the idiosyncratic component (e_t) captures series-specific dynamics. The factors f_t are assumed to be uncorrelated with the errors e_t. Following Stock and Watson (2002), the idiosyncratic errors may exhibit weak correlation across variables or over time, but this correlation is limited. The critical condition is the averagability of these errors: their overall contribution becomes negligible as the cross-sectional dimension N is sufficiently large, ensuring that the estimated common factors consistently capture the dominant systematic variation in the data. This structure provides the theoretical foundation for the Diffusion Index forecasting methodology.

The key assumption of the static factor model is that a small number of common factors drives the co-movement across a large panel of macroeconomic variables, reflecting pervasive economic forces such as aggregate demand or business cycle conditions.



## Interpretation of Common Factors and Idiosyncratic Errors

The common factors (f_t) represent the latent, underlying sources of macroeconomic co-movement. Economically, they summarize aggregate economic conditions, such as the general level of activity, inflation expectations, or business cycle fluctuations, that simultaneously affect many different variables. Statistically, they function as an effective low-dimensional summary of the N-dimensional information set, condensing the complex, collective movement of the entire economy into a few interpretable components.

The idiosyncratic error term (e_t) captures variable-specific movements not explained by the common factors. Economically, these components reflect shocks or dynamics unique to individual sectors or series, such as industry-specific regulatory changes or measurement error. Statistically, the assumption of weak cross-sectional correlation for e_t ensures that as the number of variables N increases, the effects of these series-specific shocks tend to average out. This makes the factors f_t the dominant source of systematic variation and the most informative components for large-scale forecasting.



```{r}
## Estimate an Eight-Factor Model

# Data Standardization 
# Factor model estimation (PCA) requires all input series to be standardized (zero mean, unit variance) to ensure that all variables contribute equally to the calculation of the common factors, regardless of their scale.
X.scaled <- scale(X, center = TRUE, scale = TRUE)

# If any series has zero variance (e.g., if it was constant), scale() might produce NA.
# Filter out any columns that contain NA values after standardization.
X.scaled <- X.scaled[, colSums(is.na(X.scaled)) == 0]

# Run Principal Component Analysis (PCA) 
n_factors <- 8      # Define the number of common factors to extract.

# prcomp performs PCA, which is the standard estimation method for static factor models.
pca_model <- prcomp(
  X.scaled,  
  center = FALSE,   # Data has already been centered (mean=0) by scale()
  scale. = FALSE    # Data has already been scaled (variance=1) by scale()
)

# View the Factor Loadings (Lambda_hat)
head(pca_model$rotation[, 1:n_factors]) 

# Extract Estimated Factors (f_t_hat) 
# f_t_hat represents the estimated factors (Principal Components).
f_t_hat <- pca_model$x[, 1:n_factors]

# Convert the factors matrix to a tidy tibble (or data.frame) for easy use in subsequent regressions.
f_t_hat <- as_tibble(f_t_hat)
colnames(f_t_hat) <- paste0("Factor_", 1:n_factors)

print("Estimated Factors (f_t_hat) Head:")
print(head(f_t_hat))        # Inspect the first few observations of the 8 estimated factors.

```
## PCA Factor Estimation and Diffusion Index Construction

An eight-factor static factor model is estimated using the Principal Components Analysis (PCA) method over the full sample period from 1961.01 to 2019.12. Prior to factor extraction, the data x_t are standardized by centering each series to zero mean and scaling it to unit variance. This step ensures that the estimation of the factors is not dominated by variables with larger natural scales.

The estimated factors are obtained as the first eight principal components of the standardized data matrix. These factors capture the dominant sources of common variation across the large panel of macroeconomic time series.

Following the diffusion index framework of Stock and Watson (2002), the estimated factors provide a low-dimensional summary of the information contained in the high-dimensional dataset. They are subsequently used as diffusion indexes in the empirical analysis.


```{r}
## R^2 Regression Analysis and Plotting

# Combine Standardized Data (X.scaled) and Estimated Factors (f_t_hat)
# The data needs to be aligned for the regression: x_i,t = alpha_i + beta_i' * f_t_tilde + u_i,t
data_for_reg <- bind_cols(as_tibble(X.scaled, rownames = NA), f_t_hat)

# Extract the names of the standardized input variables (x_i).
x_vars <- colnames(X.scaled)
N <- length(x_vars)        # Total number of variables in the panel (excluding Group 7 and NAs).

# Initialize a vector to store the R^2 value for each of the N regressions.
R_squared_values <- numeric(N)

# Construct the Right-Hand Side (RHS) of the regression formula (the factor terms).
formula_str <- paste0(paste(colnames(f_t_hat), collapse = " + "))

# Loop through every variable x_i,t and run a regression against the 8 factors
for (i in 1:N) {
  
  # Handle variable names that contain spaces or special characters (e.g., "S&P 500"). These names must be enclosed in backticks (`) for the 'lm' function to parse them correctly as the dependent variable.
  quoted_y_var <- paste0("`", x_vars[i], "`") 
  
  # Construct the full regression formula dynamically: '`S&P 500` ~ Factor_1 + ... + Factor_8'
  current_formula <- as.formula(paste(quoted_y_var, "~", formula_str))
  
  # linear regression (Ordinary Least Squares, OLS)
  model <- lm(current_formula, data = data_for_reg)
  
  # Extract the Coefficient of Determination (R^2)
  R_squared_values[i] <- summary(model)$r.squared
}

print(summary(R_squared_values))

```

```{r}
# Plotting the R^2 Distribution

# Create a data frame (tibble) to facilitate plotting with ggplot2.
R2_df <- tibble(
  Index = 1:N,
  R2 = R_squared_values,
  Variable = x_vars         # Store the variable names for reference/labeling if needed
)

# Use ggplot2 to create a bar plot, visualizing the explanatory power of the 8 factors
# across all macroeconomic series (mimicking McCracken & Ng (2016) Figure 1).
plot_R2 <- ggplot(R2_df, aes(x = Index, y = R2)) +
  # Use geom_bar with 'identity' stat to plot the R2 values as bar heights.
  geom_bar(stat = "identity", fill = "#377eb8", alpha = 0.8) +  # Use a deep blue color scheme
  
  # Set professional labels using expression() for proper rendering of R^2 notation.
  labs(
    title = expression(paste(R^2, " from Regressing ", x[i]~" on 8 Estimated Factors")),
    x = "Variable Index (i)",       # The index simply runs from 1 to N (number of variables)
    y = expression(R[i]^2)          # R-squared symbol using LaTeX rendering
  ) +
  theme_minimal() +
  
  # Adjust the y-axis range for clarity, fixing limits from 0 to 1.
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +
  
  # Add a horizontal dashed line to show the average explanatory power across all variables.
  geom_hline(yintercept = mean(R_squared_values), 
             linetype = "dashed", 
             color = "#e41a1c", 
             linewidth = 0.8) +     # Use a contrasting red dashed line for emphasis
  
  # Keep the title/axis label for context.
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

print(plot_R2)

```


## Explanatory Power of the Estimated Factors

To assess the explanatory power of the estimated common factors, each standardized macroeconomic variable is regressed on the eight estimated factors over the full sample period. For each variable, the coefficient of determination (R²) measures the proportion of its variation explained by the common factors.

This figure plots the R² values across variables and reveals substantial heterogeneity in explanatory power, which is typical in large macroeconomic panels. For a large subset of series, the estimated factors explain a substantial share of variation, with many R² values exceeding 0.6. The average R² across all variables is approximately 0.486, with a median of 0.487, indicating that nearly half of the variation in a typical series is captured by the common factors. This provides strong empirical support for the core assumption of the static factor model: the presence of pervasive common components driving co-movement across macroeconomic variables.

At the same time, some variables exhibit very low R² values, with a minimum close to zero. For these series, factor-based explanations are weak, suggesting that their fluctuations are primarily driven by idiosyncratic shocks and variable-specific dynamics rather than by aggregate macroeconomic forces. 

Overall, the distribution of R² values indicates that while common factors account for a substantial share of aggregate variation, their importance differs markedly across individual variables.


```{r}
# Calculating and Plotting Cumulative Variance Explained (Model Diagnostic) 

# Calculate the Variance Explained by Each Principal Component (Factor)
variance_explained <- pca_model$sdev^2
total_variance <- sum(variance_explained)

# Calculate the proportion of total variance explained by each component.
proportion_variance <- variance_explained / total_variance

# Calculate the Cumulative Proportion of Variance Explained
# This shows how much total variation is captured as more factors are included.
cumulative_variance <- cumsum(proportion_variance)

# Construct a Table for the First 8 Factors
n_factors <- 8
variance_df <- data.frame(
    Factor = 1:n_factors,
    Proportion_Variance = proportion_variance[1:n_factors],
    Cumulative_Variance = cumulative_variance[1:n_factors]
)

print("Variance Explained by Factors:")
print(variance_df)

# Plotting the Cumulative Explanatory Power (Elbow Plot)
plot_cumulative_variance <- ggplot(variance_df, aes(x = Factor, y = Cumulative_Variance)) +
    # Use a line and point plot for the cumulative data
    geom_line(color = "#4daf4a", linewidth = 1.2) + 
    geom_point(color = "#4daf4a", size = 3) +
    labs(
        title = "Cumulative Proportion of Variance Explained by Factors",
        x = "Number of Factors (r)",
        y = "Cumulative Variance Explained"
    ) +
    theme_minimal() +
    scale_x_continuous(breaks = 1:n_factors) +     # Ensure x-axis ticks are integers
    scale_y_continuous(limits = c(0, 1))

print(plot_cumulative_variance)


# F1's Proportion of Variance Explained 
F1_prop <- variance_df$Proportion_Variance[1] * 100 
print(F1_prop) 

# Cumulative Proportion of Variance Explained by F1 through F8 
F8_cum_prop <- variance_df$Cumulative_Variance[n_factors] * 100 
print(F8_cum_prop)

```
## Cumulative Variance Explained by the Factors

Additional insight into the factor structure is obtained by examining the cumulative proportion of variance explained by the principal components. Figure 3.2 illustrates this measure. The first factor alone explains approximately 17.18% of the total variance, reflecting a dominant aggregate economic force. As additional factors are included, the cumulative variance explained increases steadily.

Using eight factors, the cumulative proportion of variance explained reaches approximately 48.60%. This indicates that a relatively small number of factors captures a substantial share of the systematic variation in the high-dimensional dataset. The result highlights the effectiveness of the factor model as a dimensionality-reduction tool and supports its use in subsequent diffusion index analysis.

```{r}
## Rolling Window Forecasting and Diffusion Index Model Comparison

#Initialization of Loop Parameters and Storage
N.loops <- 16 * 12 + 1      # Number of loops for the out-of-sample forecast initiation
obs <- nrow(X.clean)        # Total number of observations (time points) in the sample (T)

# Model 1: Autoregressive (AR) Model
# Model 2: Diffusion Index (DI) Model (AR + Factors)
nmod = 2                    # Number of models

# Storage Matrices for Forecasts (y_t+h_hat)
y.hat.h6 <- matrix(NA, obs, nmod)   # Forecasts for horizon h = 6 months
y.hat.h12 <- matrix(NA, obs, nmod)  # Forecasts for horizon h = 12 months
y.hat.h24 <- matrix(NA, obs, nmod)  # Forecasts for horizon h = 24 months

# Storage Matrices for Mean Squared Error (MSE) 
mse.h6 <- matrix(NA, obs, nmod)
mse.h12 <- matrix(NA, obs, nmod)
mse.h24 <- matrix(NA, obs, nmod)
```

```{r}
count <- 1       # Counter for the iteration number of the out-of-sample period
t.1 <- 30        # Start date for estimation (1963.06). This is the earliest date where all required lags (e.g., y_t-6) are available.

# Prepare Target Variable (y_t) and its Lags (y_t-1 to y_t-6)
y.lags <- matrix(NA, obs, 7)
y.lags[, 1] <- y[1:obs]                         # y_t (Target/LHS in AR)
y.lags[2:obs, 2] <- y[1:(obs-1)]                # y_t-1
y.lags[3:obs, 3] <- y[1:(obs-2)]                # y_t-2
y.lags[4:obs, 4] <- y[1:(obs-3)]                # y_t-3
y.lags[5:obs, 5] <- y[1:(obs-4)]                # y_t-4
y.lags[6:obs, 6] <- y[1:(obs-5)]                # y_t-5
y.lags[7:obs, 7] <- y[1:(obs-6)]                # y_t-6

y_lag_names <- paste0("y_L", 0:6)               # y_L0, y_L1, ..., y_L6 (where L0 is y_t)
factor_lag_names <- c()

for (f in 1:4) {                                # 4 Factors (F1, F2, F3, F4)
    for (l in 1:4) {                            # 4 Lags (L0, L1, L2, L3)
        factor_lag_names <- c(factor_lag_names, paste0("F", f, "_L", l-1))
    }
}
all_DI_names <- c(y_lag_names, factor_lag_names)
colnames(y.lags) <- y_lag_names                 # Assign column names to the y_lags matrix

for (T in seq(492,684)){ 
  # T=492 corresponds to 1999.04. The first forecast is for T+h.
print(paste("Sample ",count,"Forecast date, T =",dates[T]))
  # Prepare LHS (Target Variables: y_t+h) 
    y6.temp <- y6[(t.1 + 6):T]     # y_t+6 (for h=6 model)
    y12.temp <- y12[(t.1 + 12):T]  # y_t+12 (for h=12 model)
    y24.temp <- y24[(t.1 + 24):T]  # y_t+24 (for h=24 model)
    # sample size, differs for different h
    n6 <- length(y6.temp)
    n12 <- length(y12.temp)
    n24 <- length(y24.temp) 
    
    # last observation used for estimation
    t.last.6 <- t.1 + n6 - 1 
    t.last.12 <- t.1 + n12 - 1 
    t.last.24 <- t.1 + n24 - 1

    # prepare RHS variables
    y6.lags <- y.lags[t.1:t.last.6, ] 
    y12.lags <- y.lags[t.1:t.last.12, ] 
    y24.lags <- y.lags[t.1:t.last.24, ] 

    # Dynamic Factor Estimation .
    X.temp <- X[1:T, ]             # Use all data up to T for factor estimation
    f.temp <- rpca(X.temp, 4, standardize = TRUE, tau = 0)
    
    # Extract the first 4 estimated factors (f_t_hat)
    f.hat.temp <- as.data.frame(f.temp$F[, 1:4]) 
    
    # Prepare RHS (Factor Lags) 
    # Store Factor 1 for t, t-1, t-2, and t-3 (max lag being used is 3)
    f.hat.1.lags <- matrix(NA, T, 4)               # for factor 1
    f.hat.1.lags[, 1] <- f.hat.temp[1:T, 1]        # f_1,t
    f.hat.1.lags[2:T, 2] <- f.hat.temp[1:(T-1), 1] # f_1,t-1
    f.hat.1.lags[3:T, 3] <- f.hat.temp[1:(T-2), 1] # f_1,t-2
    f.hat.1.lags[4:T, 4] <- f.hat.temp[1:(T-3), 1] # f_1,t-3
    
    # Store Factor 2 for t, t-1, t-2 and t-3
    f.hat.2.lags <- matrix(NA, T, 4)
    f.hat.2.lags[, 1] <- f.hat.temp[1:T, 2]
    f.hat.2.lags[2:T, 2] <- f.hat.temp[1:(T-1), 2]
    f.hat.2.lags[3:T, 3] <- f.hat.temp[1:(T-2), 2]
    f.hat.2.lags[4:T, 4] <- f.hat.temp[1:(T-3), 2]

    # Store Factor 3 for t, t-1, t-2 and t-3
    f.hat.3.lags <- matrix(NA, T, 4)
    f.hat.3.lags[, 1] <- f.hat.temp[1:T, 3]
    f.hat.3.lags[2:T, 2] <- f.hat.temp[1:(T-1), 3]
    f.hat.3.lags[3:T, 3] <- f.hat.temp[1:(T-2), 3]
    f.hat.3.lags[4:T, 4] <- f.hat.temp[1:(T-3), 3]

    # Store Factor 4 for t, t-1, t-2 and t-3
    f.hat.4.lags <- matrix(NA, T, 4)
    f.hat.4.lags[, 1] <- f.hat.temp[1:T, 4]
    f.hat.4.lags[2:T, 2] <- f.hat.temp[1:(T-1), 4]
    f.hat.4.lags[3:T, 3] <- f.hat.temp[1:(T-2), 4]
    f.hat.4.lags[4:T, 4] <- f.hat.temp[1:(T-3), 4]
    
    # Combine all 4 factors, each with 4 lags (Total 16 predictor columns)
    f.hat.lags <- cbind(f.hat.1.lags, f.hat.2.lags, f.hat.3.lags, f.hat.4.lags)
    f.hat.6.lags <- f.hat.lags[t.1:t.last.6,] 
    f.hat.12.lags <- f.hat.lags[t.1:t.last.12,] 
    f.hat.24.lags <- f.hat.lags[t.1:t.last.24,] 
    
    colnames(f.hat.lags) <- factor_lag_names   # Assign column names to the full factor lag matrix
    
    # Sanity Checks: Ensure Time Alignment
    # Verify that the LHS (y_t+h) and RHS (predictors at t) matrices have the same length (rows).
    stopifnot(length(y6.temp)  == nrow(y6.lags))
    stopifnot(length(y12.temp) == nrow(y12.lags))
    stopifnot(length(y24.temp) == nrow(y24.lags))

    stopifnot(nrow(y6.lags)  == nrow(f.hat.6.lags))
    stopifnot(nrow(y12.lags) == nrow(f.hat.12.lags))
    stopifnot(nrow(y24.lags) == nrow(f.hat.24.lags))
    
    
    ## AR6 
    # Set up data frame with all variables for the AR models
    x.T.pred <- y.lags[T, ]         # AR predictors at time T (y_t, y_t-1, ..., y_t-6)
    
    # Data setup for AR(6)
    # h=6 setup:
    temp.data.6 <- data.frame(y = y6.temp, y6.lags)   # Estimation data (LHS y_t+6, RHS y_t:y_t-6)
    temp.data.6.T <- data.frame(y = y6[T+6], t(x.T.pred))  # Forecast data (RHS at time T)
    
    # h=12 setup:
    temp.data.12 <- data.frame(y = y12.temp, y12.lags)
    temp.data.12.T <- data.frame(y = y12[T+12], t(x.T.pred))

    # h=24 setup:
    temp.data.24 <- data.frame(y = y24.temp, y24.lags)
    temp.data.24.T <- data.frame(y = y24[T+24], t(x.T.pred))
   
     # Estimate AR(6) models using the current rolling window data
    AR6.6  <- try(lm(y ~ ., data = temp.data.6), silent = TRUE)
    AR6.12 <- try(lm(y ~ ., data = temp.data.12), silent = TRUE)
    AR6.24 <- try(lm(y ~ ., data = temp.data.24), silent = TRUE)

    # Skip current iteration if any model estimation fails (e.g., singular matrix)
    if (inherits(AR6.6, "try-error") ||
        inherits(AR6.12, "try-error") ||
        inherits(AR6.24, "try-error")) {
        next
    }
    
    # Predict y_T+h and save in Column 1 (AR model)
    y.hat.h6[T+6, 1]  <- predict(AR6.6, newdata = temp.data.6.T)
    y.hat.h12[T+12, 1] <- predict(AR6.12, newdata = temp.data.12.T)
    y.hat.h24[T+24, 1] <- predict(AR6.24, newdata = temp.data.24.T)

    # Calculate Squared Forecast Error (MSE) and save in Column 1
    mse.h6[T+6, 1] <- (y.hat.h6[T+6, 1] - y6[T+6])^2
    mse.h12[T+12, 1] <- (y.hat.h12[T+12, 1] - y12[T+12])^2
    mse.h24[T+24, 1] <- (y.hat.h24[T+24, 1] - y24[T+24])^2

    #  DI (includes 6 y.lags and 4 factors with 3 lags each) 
    # Predictor variables at time T (already defined y.lags[T,] as x.T.pred)
    x.T.pred <- y.lags[T, ]
    f.T.pred <- f.hat.lags[T, ]  # Factor predictors at time T (f_t, f_t-1, ..., f_t-3)

    # Data Setup for DI Model Estimation 
    temp.data.6  <- data.frame(y = y6.temp, y6.lags, f.hat.6.lags)
    temp.data.12 <- data.frame(y = y12.temp, y12.lags, f.hat.12.lags)
    temp.data.24 <- data.frame(y = y24.temp, y24.lags, f.hat.24.lags)
   
     # Data Setup for DI Model Forecast
    temp.data.6.T  <- data.frame(t(x.T.pred), t(f.T.pred))
    temp.data.12.T <- data.frame(t(x.T.pred), t(f.T.pred))
    temp.data.24.T <- data.frame(t(x.T.pred), t(f.T.pred))
    
    # match the estimation model's RHS column names.
    colnames(temp.data.6.T)  <- colnames(temp.data.6)[-1]
    colnames(temp.data.12.T) <- colnames(temp.data.12)[-1]
    colnames(temp.data.24.T) <- colnames(temp.data.24)[-1]

    # Estimate DI models using the current rolling window data
    DI.6  <- try(lm(y ~ ., data = temp.data.6), silent = TRUE)
    DI.12 <- try(lm(y ~ ., data = temp.data.12), silent = TRUE)
    DI.24 <- try(lm(y ~ ., data = temp.data.24), silent = TRUE)

    if (inherits(DI.6, "try-error") ||
        inherits(DI.12, "try-error") ||
        inherits(DI.24, "try-error")) {
        next
    }
    
    # Predict and Save MSE (only if T+h is within the total sample obs)
   
    if (T + 6 <= obs) {
        y.hat.h6[T+6, 2] <- predict(DI.6, newdata = temp.data.6.T)
        mse.h6[T+6, 2]   <- (y.hat.h6[T+6, 2] - y6[T+6])^2
    }

    if (T + 12 <= obs) {
        y.hat.h12[T+12, 2] <- predict(DI.12, newdata = temp.data.12.T)
        mse.h12[T+12, 2]   <- (y.hat.h12[T+12, 2] - y12[T+12])^2
    }

    if (T + 24 <= obs) {
        y.hat.h24[T+24, 2] <- predict(DI.24, newdata = temp.data.24.T)
        mse.h24[T+24, 2]   <- (y.hat.h24[T+24, 2] - y24[T+24])^2
    }

    count = count + 1
}



#Final Diagnostic Checks ---
# Count the number of valid (non-NA) out-of-sample forecasts generated.
sum(!is.na(mse.h6[,2]))
sum(!is.na(mse.h12[,2]))
sum(!is.na(mse.h24[,2]))


```

```{r}
# Final Out-of-Sample Performance Evaluation

# Calculate Mean Squared Forecast Error (MSFE), which is the average of the squared forecast errors (mse.h), ignoring initial NA values (na.rm = TRUE).

# Column 1 (AR Model), Column 2 (DI Model)
msfe.h6 <- colMeans(mse.h6, na.rm = TRUE)   # MSFE for h = 6 months
msfe.h12 <- colMeans(mse.h12, na.rm = TRUE) # MSFE for h = 12 months
msfe.h24 <- colMeans(mse.h24, na.rm = TRUE) # MSFE for h = 24 months

# Calculate Relative MSFE (Diffusion Index vs. AR Benchmark)
# A value < 1 indicates the Diffusion Index model outperforms the AR model.

# msfe.h[2] is the DI Model MSFE; msfe.h[1] is the AR Model MSFE.
rel.h6  <- msfe.h6[2]  / msfe.h6[1]        # Relative MSFE for h=6
rel.h12 <- msfe.h12[2] / msfe.h12[1]       # Relative MSFE for h=12
rel.h24 <- msfe.h24[2] / msfe.h24[1]       # Relative MSFE for h=24


results <- data.frame(
    h = c(6, 12, 24),
    Relative_MSFE = c(rel.h6, rel.h12, rel.h24),
    AR_MSFE = c(msfe.h6[1], msfe.h12[1], msfe.h24[1]), 
    DI_MSFE = c(msfe.h6[2], msfe.h12[2], msfe.h24[2]) 
)

print(results)
```
## Forecast Accuracy Evaluation

Forecast accuracy is evaluated using mean squared forecast errors (MSFE), consistent with the quadratic loss function commonly adopted in the diffusion index literature (e.g. Stock and Watson, 2002; Bai and Ng, 2008). While alternative loss functions could be considered, MSFE provides a natural benchmark for comparing factor-augmented and autoregressive models in this setting.

This table reports the out-of-sample forecasting performance of the AR(6) benchmark and the diffusion index (DI) model at horizons h = 6, 12, 24. Forecast accuracy is evaluated using MSFE and compared via relative MSFE, defined as the ratio of the DI MSFE to the AR MSFE.

Across all forecast horizons, the relative MSFE exceeds one, indicating that over the full out-of-sample period the diffusion index model produces larger forecast errors than the AR(6) benchmark. In particular, at the 24-month horizon, the DI model’s MSFE is approximately 15% higher than that of the AR model. These results indicate that augmenting the autoregressive benchmark with estimated common factors does not improve forecast accuracy for the target variable considered.

This outcome is economically intuitive. First, the target variable exhibits strong own-dynamics, which are already well captured by its autoregressive structure. As a result, much of the relevant predictive information is contained in its own lagged values. Second, the diffusion index model introduces a large number of additional regressors through factor lags, increasing estimation uncertainty in a rolling-window setting. When the incremental predictive content of the estimated factors is limited, this additional complexity can deteriorate out-of-sample performance.

Finally, the factor estimates themselves may contain limited incremental information after excluding interest rate and exchange rate variables and applying strict missing-value screening. Overall, the informational gains from including common factors are outweighed by the increase in estimation noise and model complexity. Consequently, in the full-sample evaluation, the simpler and more parsimonious AR(6) specification delivers superior out-of-sample forecasting performance in this application.



## Comparison with Stock and Watson (2002) and Conclusion

The out-of-sample forecasting results obtained in this project can be meaningfully compared with those reported by Stock and Watson (2002) in their Tables 1 and 2. In those tables, Stock and Watson evaluate diffusion index forecasts for a range of U.S. macroeconomic variables and show that factor-augmented models often outperform simple autoregressive benchmarks, particularly at medium- to long-term forecast horizons. However, they also emphasize that the gains from diffusion index models are heterogeneous across variables and horizons, and that improvements are not universal.

In this respect, the results of the present project share important similarities with those of Stock and Watson (2002). First, both analyses highlight that the forecasting performance of diffusion index models is variable-specific and context-dependent. While Stock and Watson report substantial gains for some macroeconomic series, they also document cases in which autoregressive models remain competitive. Similarly, in this project, the diffusion index model explains a large fraction of in-sample variation, as reflected in the $R^2$ results, but this explanatory power does not necessarily translate into superior out-of-sample forecasts for the target variable.

At the same time, there are notable differences between the findings. In contrast to Stock and Watson (2002), where diffusion index models frequently improve forecast accuracy relative to autoregressive benchmarks, the results in this project indicate that the AR(6) model outperforms the diffusion index specification over the full out-of-sample period at all horizons considered. This difference likely reflects the strong autoregressive dynamics of the specific target variable examined here, as well as increased estimation uncertainty arising from the inclusion of multiple factor lags in a rolling-window framework.

Overall, the comparison suggests that the results of this project do not contradict the conclusions of Stock and Watson (2002), but instead reinforce their central message: diffusion index models can be powerful forecasting tools under appropriate conditions, yet their performance depends critically on the variable being forecast, the forecast horizon, and the balance between informational gains and estimation uncertainty. In this application, that balance favors a more parsimonious autoregressive specification.


```{r}
# DETERMINING THE OPTIMAL NUMBER OF FACTORS (r) 
# Based on the Information Criteria (IC) proposed by Bai and Ng (2002).

# Load the cleaned data matrix.
X <- as.matrix(X.scaled)

# Get dimensions of the data matrix
T <- nrow(X)    # Time dimension (number of observations)
N <- ncol(X)    # Cross-sectional dimension (number of variables)

# Define the maximum number of factors to test
r_max <- 10

# Initialize storage vectors for the three Bai & Ng IC variants
IC1 <- IC2 <- IC3 <- rep(NA, r_max)

# Start the loop to compute the IC for r = 1 to r_max
for (r in 1:r_max) {

    # Estimate Factors (F_hat) and Loadings (Lambda_hat) using PCA
    # We set center=FALSE and scale.=FALSE because the data X is already scaled (standardized).
    pca_full <- prcomp(X, center = FALSE, scale. = FALSE)

    # Extract the first 'r' estimated factors (scores)
    F_hat <- pca_full$x[, 1:r, drop = FALSE]
    
    # Extract the corresponding factor loadings (rotation matrix)
    Lambda_hat <- pca_full$rotation[, 1:r, drop = FALSE]

    # Reconstruct X and Calculate Residual Variance (V_r)
    # The reconstructed data matrix (X_hat = F_hat * Lambda_hat')
    X_hat <- F_hat %*% t(Lambda_hat)

    # V_r is the average variance of the residuals (unexplained variation in X)
    V_r <- mean((X - X_hat)^2)

    # Apply Bai & Ng (2002) IC Formulas
    # IC1: Penalty based on the log of the dimension ratio.
    IC1[r] <- log(V_r) + 
        r * (N + T) / (N * T) * log(N * T / (N + T))

    # IC2: Penalty based on the log of the minimum dimension. (More conservative than IC1)
    IC2[r] <- log(V_r) + 
        r * (N + T) / (N * T) * log(min(N, T))

    # IC3: Penalty based on the minimum dimension. (Can lead to a larger factor number)
    IC3[r] <- log(V_r) + 
        r * log(min(N, T)) / min(N, T)
}

# Compile results into a data frame for presentation
IC_table <- data.frame(
    r = 1:r_max,
    IC1 = IC1,
    IC2 = IC2,
    IC3 = IC3
)

print(IC_table)


# Identify the recommended number of factors (r) for each criterion
# The optimal r is the one that minimizes the IC value.
print("Recommended number of factors (r):")
cat("IC1:", which.min(IC_table$IC1), "\n")
cat("IC2:", which.min(IC_table$IC2), "\n")
cat("IC3:", which.min(IC_table$IC3), "\n")

```

Determining the Number of Factors: Information Criteria and Scree Plot

The number of factors in a static factor model is not known a priori. While the baseline specification in the assignment uses four factors, it is important to assess whether this choice is supported by the data or whether an alternative factor dimension may be more appropriate.

To address this issue, the information criteria proposed by Bai and Ng (2002) are computed using the full sample. These criteria provide data-driven guidance on the factor dimension by balancing model fit against parsimony. The results indicate some dispersion across the criteria: IC1 and IC2 suggest five factors, while IC3 suggests a larger specification with eight factors. Such variation across criteria is common in empirical applications and reflects the trade-off inherent in factor model selection.

Given the forecasting focus of this exercise and the evidence that adding more predictors does not necessarily improve short- and medium-horizon forecast accuracy, the four-factor specification is maintained as the baseline for the empirical analysis. Specifications with a larger number of factors are examined subsequently to assess whether alternative factor dimensions provide incremental benefits.

Finally, it should be noted that the information criteria are computed using the full sample and are therefore intended to provide indicative guidance on factor dimensionality rather than to determine a forecasting-optimal choice.

```{r}
# Scree Plot

# Perform Principal Component Analysis (PCA) using the standardized data (X.scaled).
# We set center=FALSE and scale.=FALSE because X.scaled is already standardized
# (zero mean, unit variance) from the data preparation stage.
pca_full <- prcomp(X.scaled, center = FALSE, scale. = FALSE) 

# Extract the eigenvalues (squared standard deviations of the principal components).
# Each eigenvalue represents the variance captured by that factor.
eigenvalues <- pca_full$sdev^2

# Plot the Scree Plot to visually identify the "elbow" point.
plot(
    eigenvalues[1:10],      # Plotting the first 10 eigenvalues
    type = "b",             # Plotting both points and lines
    pch = 19,               # Solid circles for points
    xlab = "Factor Number (r)",
    ylab = "Eigenvalue",
    main = "Scree Plot of Principal Components (Eigenvalues)"
)


# Mark the baseline four-factor specification used in the baseline model for reference.
abline(v = 4, lty = 2, col = "red")

```
Scree Plot Analysis

The scree plot provides a graphical diagnostic of the factor structure by plotting the eigenvalues associated with the principal components of the standardized data matrix. Each eigenvalue measures the amount of variance explained by the corresponding factor, and the rate at which eigenvalues decline offers insight into the marginal contribution of additional factors.

Figure X displays the eigenvalues for the first ten principal components. Several features are immediately apparent. First, the first factor explains a substantially larger share of total variation than any subsequent factor, indicating the presence of a dominant common component affecting a broad range of macroeconomic variables. Second, eigenvalues decline rapidly for the next few factors, suggesting that these factors also capture meaningful common variation. However, after the fourth factor, the decline becomes noticeably flatter, indicating that additional factors contribute relatively little incremental explanatory power.

The flattening of the scree plot beyond the fourth factor suggests that the dominant systematic variation in the data is concentrated in the first few factors. Factors beyond this point primarily capture smaller-scale or more variable-specific components, rather than pervasive macroeconomic forces. The red dashed line marking four factors lies close to the visual “elbow” of the scree plot, reinforcing the interpretation that the marginal gains from including additional factors diminish substantially beyond this point.


Comparison with Information Criteria

The scree plot evidence complements, but does not fully coincide with, the results from the Bai and Ng (2002) information criteria. While the information criteria suggest that a larger number of factors—ranging from five to eight—may improve in-sample fit, the scree plot indicates that the incremental explanatory power of additional factors falls sharply after the first few components.

This divergence is not unusual in empirical factor model applications. Information criteria are designed to balance fit and parsimony using asymptotic approximations and are sensitive to sample size and cross-sectional dimension. In contrast, the scree plot provides a visual, model-free diagnostic that emphasizes the concentration of variance across factors. Together, these tools highlight the trade-off between capturing additional variation and avoiding the inclusion of weak factors with limited marginal contribution.


Implications for Forecasting and Model Extension

From a forecasting perspective, the presence of weak additional factors raises an important concern. Including factors with low marginal explanatory power may introduce noise and increase estimation uncertainty, particularly in a rolling-window forecasting framework where parameters must be repeatedly re-estimated. As a result, improvements in in-sample fit suggested by the information criteria may not translate into superior out-of-sample forecasting performance.

Taken together, the scree plot and the information criteria do not point to a unique optimal factor dimension. Instead, they motivate a direct empirical evaluation of alternative factor specifications. In particular, while the baseline four-factor model appears to capture the dominant common variation in the data, the information criteria suggest that an additional factor may contain incremental information. This creates a natural setting for extending the diffusion index model to include a fifth factor and assessing whether the potential gains in explanatory power lead to measurable improvements in forecast accuracy.


In the following section, the forecasting performance of a five-factor diffusion index model is therefore examined and compared with that of the baseline four-factor specification.

```{r}
## Diffusion Index (DI) model with 5 factors 
# A robustness check using the factor number suggested by IC1/IC2.

# Assuming 'obs', 't.1', and 'T.seq' were defined globally.
obs_DI5 <- obs           # Total number of observations (rows in y, X)
t.1_DI5 <- t.1           # First observation index used for forecasting (end of initial estimation sample)
T.seq_DI5 <- 492:684     # Sequence of final observation indices (T) for the rolling window
nmod_DI5 <- 2            # Number of models (AR(6) and DI(5,4,6))

# Define the number of factors (r) for this specific robustness check
r_DI5 <- 5
# Define the number of factor lags (p) (Assuming p=4, L0 to L3)
p_DI5 <- 4

# Storage 
# Store forecast values (y.hat) and Mean Squared Errors (mse) for the AR(6) and DI(5) models
y.hat.h6_DI5 <- matrix(NA, obs_DI5, nmod_DI5)
y.hat.h12_DI5 <- matrix(NA, obs_DI5, nmod_DI5)
y.hat.h24_DI5 <- matrix(NA, obs_DI5, nmod_DI5)

mse.h6_DI5 <- matrix(NA, obs_DI5, nmod_DI5)
mse.h12_DI5 <- matrix(NA, obs_DI5, nmod_DI5)
mse.h24_DI5 <- matrix(NA, obs_DI5, nmod_DI5)

# Y Lags 
# Check if y.lags (for AR(6)) was already created in a previous step 
if (!exists("y.lags")) {
    # If not, create the matrix of y and its 6 lags (y_t-1 to y_t-6)
    y.lags <- matrix(NA, obs_DI5, 7)
    y.lags[,1] <- y[1:obs_DI5]
    y.lags[2:obs_DI5,2] <- y[1:(obs_DI5-1)]
    y.lags[3:obs_DI5,3] <- y[1:(obs_DI5-2)]
    y.lags[4:obs_DI5,4] <- y[1:(obs_DI5-3)]
    y.lags[5:obs_DI5,5] <- y[1:(obs_DI5-4)]
    y.lags[6:obs_DI5,6] <- y[1:(obs_DI5-5)]
    y.lags[7:obs_DI5,7] <- y[1:(obs_DI5-6)]
}

# Rolling Loop 
count_DI5 <- 1

for (T in T.seq_DI5) {

    # Print progress occasionally
    if (count_DI5 %% 25 == 1) {
        print(paste("DI(5) forecasting - Sample", count_DI5, "T =", dates[T]))
    }
    count_DI5 <- count_DI5 + 1
    
    # LHS preparation (Dependent variables y_t+h)
    # Slice the series y_t+h based on the current end of the estimation sample (T)
    y6.temp <- y6[(t.1_DI5 + 6):T]
    y12.temp <- y12[(t.1_DI5 + 12):T]
    y24.temp <- y24[(t.1_DI5 + 24):T]

    # Calculate lengths and indices for the estimation windows
    n6 <- length(y6.temp)
    n12 <- length(y12.temp)
    n24 <- length(y24.temp)

    t.last.6 <- t.1_DI5 + n6 - 1
    t.last.12 <- t.1_DI5 + n12 - 1
    t.last.24 <- t.1_DI5 + n24 - 1

    # RHS Y-lags (Independent variables: y_t-1 to y_t-6)
    y6.lags <- y.lags[t.1_DI5:t.last.6, ]
    y12.lags <- y.lags[t.1_DI5:t.last.12, ]
    y24.lags <- y.lags[t.1_DI5:t.last.24, ]

    # AR(6) (Benchmark)
    # Predictor variables at time T for out-of-sample forecast
    x.T.pred <- y.lags[T, ]

    # Prepare data frames for lm estimation (AR(6))
    temp.data.6 <- data.frame(y = y6.temp, y6.lags)
    temp.data.6.T <- data.frame(y = y6[T+6], t(x.T.pred)) # Prediction data frame

    temp.data.12 <- data.frame(y = y12.temp, y12.lags)
    temp.data.12.T <- data.frame(y = y12[T+12], t(x.T.pred))

    temp.data.24 <- data.frame(y = y24.temp, y24.lags)
    temp.data.24.T <- data.frame(y = y24[T+24], t(x.T.pred))

    # Estimate AR(6) models using the rolling window
    AR6.6 <- try(lm(y ~ ., data = temp.data.6), silent = TRUE)
    AR6.12 <- try(lm(y ~ ., data = temp.data.12), silent = TRUE)
    AR6.24 <- try(lm(y ~ ., data = temp.data.24), silent = TRUE)

    if (inherits(AR6.6, "try-error") ||
        inherits(AR6.12, "try-error") ||
        inherits(AR6.24, "try-error")) {
        next
    }

    # Store AR(6) forecasts (Column 1)
    y.hat.h6_DI5[T+6, 1] <- predict(AR6.6, temp.data.6.T)
    y.hat.h12_DI5[T+12, 1] <- predict(AR6.12, temp.data.12.T)
    y.hat.h24_DI5[T+24, 1] <- predict(AR6.24, temp.data.24.T)

    # Store AR(6) MSEs (Column 1)
    mse.h6_DI5[T+6, 1] <- (y.hat.h6_DI5[T+6, 1] - y6[T+6])^2
    mse.h12_DI5[T+12, 1] <- (y.hat.h12_DI5[T+12, 1] - y12[T+12])^2
    mse.h24_DI5[T+24, 1] <- (y.hat.h24_DI5[T+24, 1] - y24[T+24])^2

    
    # DI model with 5 factors 
    
    # Estimate 5 Factors (F_hat) using the current estimation window (1:T)
    X.temp <- as.matrix(X[1:T, ])
    # rpca must be called with r_DI5 = 5 factors
    f.temp <- rpca(X.temp, r_DI5, standardize = TRUE, tau = 0)
    f.hat.temp <- as.data.frame(f.temp$F[, 1:r_DI5]) # Extract the first 5 factors

    # Build Factor Lags (F_t, F_t-1, F_t-2, F_t-3) for all 5 factors
    f.hat.lags <- NULL
    for (k in 1:r_DI5) {                # Loop runs from 1 to 5
        fk.lags <- matrix(NA, T, p_DI5) # p_DI5 = 4 lags (L0 to L3)
        fk.lags[,1] <- f.hat.temp[1:T, k]
        fk.lags[2:T,2] <- f.hat.temp[1:(T-1), k]
        fk.lags[3:T,3] <- f.hat.temp[1:(T-2), k]
        fk.lags[4:T,4] <- f.hat.temp[1:(T-3), k]
        f.hat.lags <- cbind(f.hat.lags, fk.lags) # Total columns: 5 factors * 4 lags = 20
    }

    # Slice the factor lags based on the rolling window end points
    f.hat.6.lags <- f.hat.lags[t.1_DI5:t.last.6, ]
    f.hat.12.lags <- f.hat.lags[t.1_DI5:t.last.12, ]
    f.hat.24.lags <- f.hat.lags[t.1_DI5:t.last.24, ]

    # Predictor rows at time T
    f.T.pred <- f.hat.lags[T, ]

    # 3. Prepare DI Datasets (y-lags + f-lags)
    # y~x+f where x are the y-lags and f are the factor lags
    temp.data.6 <- data.frame(y = y6.temp, x = y6.lags, f = f.hat.6.lags)
    temp.data.6.T <- data.frame(y = y6[T+6], x = t(x.T.pred), f = t(f.T.pred))

    temp.data.12 <- data.frame(y = y12.temp, x = y12.lags, f = f.hat.12.lags)
    temp.data.12.T <- data.frame(y = y12[T+12], x = t(x.T.pred), f = t(f.T.pred))

    temp.data.24 <- data.frame(y = y24.temp, x = y24.lags, f = f.hat.24.lags)
    temp.data.24.T <- data.frame(y = y24[T+24], x = t(x.T.pred), f = t(f.T.pred))

    # Estimate DI(5,4,6) models
    DI5.6 <- try(lm(y ~ ., data = temp.data.6), silent = TRUE)
    DI5.12 <- try(lm(y ~ ., data = temp.data.12), silent = TRUE)
    DI5.24 <- try(lm(y ~ ., data = temp.data.24), silent = TRUE)

    # 4. Store DI(5) Forecasts and MSEs (Column 2)
    
    if (!inherits(DI5.6, "try-error") && (T + 6 <= obs_DI5)) {
        y.hat.h6_DI5[T+6, 2] <- predict(DI5.6, temp.data.6.T)
        mse.h6_DI5[T+6, 2] <- (y.hat.h6_DI5[T+6, 2] - y6[T+6])^2
    }

    if (!inherits(DI5.12, "try-error") && (T + 12 <= obs_DI5)) {
        y.hat.h12_DI5[T+12, 2] <- predict(DI5.12, temp.data.12.T)
        mse.h12_DI5[T+12, 2] <- (y.hat.h12_DI5[T+12, 2] - y12[T+12])^2
    }

    if (!inherits(DI5.24, "try-error") && (T + 24 <= obs_DI5)) {
        y.hat.h24_DI5[T+24, 2] <- predict(DI5.24, temp.data.24.T)
        mse.h24_DI5[T+24, 2] <- (y.hat.h24_DI5[T+24, 2] - y24[T+24])^2
    }
}

# --- 4) Summarize Results (DI5/AR) ---
# Calculate Mean Squared Forecast Errors (MSFE)
msfe.h6_DI5 <- colMeans(mse.h6_DI5, na.rm = TRUE)
msfe.h12_DI5 <- colMeans(mse.h12_DI5, na.rm = TRUE)
msfe.h24_DI5 <- colMeans(mse.h24_DI5, na.rm = TRUE)

# Calculate Relative MSFE (DI5 MSFE / AR(6) MSFE)
rel.h6_DI5 <- msfe.h6_DI5[2] / msfe.h6_DI5[1]
rel.h12_DI5 <- msfe.h12_DI5[2] / msfe.h12_DI5[1]
rel.h24_DI5 <- msfe.h24_DI5[2] / msfe.h24_DI5[1]

# Display results in a table
results_DI5 <- data.frame(
    h = c(6, 12, 24), 
    DI_MSFE = c(msfe.h6[2], msfe.h12[2], msfe.h24[2]),
    AR6_MSFE = c(msfe.h6_DI5[1], msfe.h12_DI5[1], msfe.h24_DI5[1]),
    DI5_MSFE = c(msfe.h6_DI5[2], msfe.h12_DI5[2], msfe.h24_DI5[2]),
    Relative_MSFE = c(rel.h6, rel.h12, rel.h24),
    DI5_Relative_MSFE = c(rel.h6_DI5, rel.h12_DI5, rel.h24_DI5)
)

print(results_DI5)

# (Optional) Show counts to confirm that a full set of non-NA forecasts was generated
print(c(
    nonNA_h6 = sum(!is.na(mse.h6_DI5[,2])),
    nonNA_h12 = sum(!is.na(mse.h12_DI5[,2])),
    nonNA_h24 = sum(!is.na(mse.h24_DI5[,2]))
))


```

Forecasting Performance with Five Factors

This section extends the forecasting analysis by examining whether the out-of-sample results are sensitive to the number of factors included in the diffusion index (DI) model. In particular, it evaluates whether increasing the factor dimension beyond the baseline four-factor specification leads to improved forecast accuracy.

As discussed before, the information criteria proposed by Bai and Ng (2002) provide mixed guidance on the appropriate factor dimension. While IC1 and IC2 suggest five factors and IC3 suggests a larger specification, the scree plot indicates that the dominant common variation in the data is concentrated in the first few factors, with diminishing marginal contributions beyond the fourth factor. Motivated by these diagnostics, an alternative diffusion index model with five factors is estimated and evaluated within the same rolling forecasting framework as in the baseline specification.

Forecast accuracy is assessed using mean squared forecast errors (MSFE) and compared to both the autoregressive benchmark and the four-factor diffusion index model. The results show that the five-factor diffusion index model performs worse than the AR(6) benchmark across all forecast horizons. Relative MSFE values exceed one for all horizons, indicating that augmenting the autoregressive model with five factors leads to forecast errors between approximately 5% and 14% higher than those of the AR(6) model.

A comparison between the five-factor and four-factor diffusion index models reveals a more nuanced pattern. At the short horizon (h = 6), the inclusion of a fifth factor further deteriorates forecast performance, with the MSFE increasing by roughly 4% relative to the four-factor model. At the medium and long horizons (h = 12 and h = 24), the five-factor specification delivers marginal improvements relative to the four-factor model, with MSFE ratios slightly below one. However, these gains are quantitatively small and not consistent across horizons.

Overall, the results indicate that while the fifth factor may capture some additional in-sample variation—as suggested by the information criteria—this does not translate into robust improvements in out-of-sample forecast accuracy. Instead, including the additional factor increases estimation uncertainty and amplifies the risk of overfitting in the rolling-window setting, particularly at the short horizon where forecast performance is most sensitive to parameter instability.

Taken together, the comparison across models highlights a clear ranking in forecasting performance: the AR(6) benchmark consistently outperforms both factor-augmented specifications, while the four-factor diffusion index model performs better than the five-factor model at the critical short horizon and comparably at longer horizons. These findings reinforce the importance of parsimony in diffusion index forecasting. Although richer factor structures may improve variance explanation in-sample, they do not necessarily yield superior out-of-sample forecasts, especially when the target variable exhibits strong autoregressive dynamics.


```{r}
# Sub-sample Stability Check (Robust Version)
# Purpose: Assess if the DI model's relative performance vs. AR(6) is consistent across different economic periods (Pre-2010 vs. Post-2010).

# Ensure 'dates' variable is in Date format
dates_C <- dates

# If 'dates' is stored as character/factor, convert it
if (is.character(dates_C) || is.factor(dates_C)) {
    dates_C <- as.Date(as.character(dates_C))
}

# If 'dates' is still not Date (e.g., yearmon / POSIXct), try a conversion
if (!inherits(dates_C, "Date")) {
    dates_C <- as.Date(dates_C)
}

# Define the split date and find corresponding indices
split_date <- as.Date("2010-01-01")

# Indices for forecasts with a target date BEFORE 2010-01-01
idx_pre_all <- which(dates_C < split_date) 
# Indices for forecasts with a target date ON or AFTER 2010-01-01
idx_post_all <- which(dates_C >= split_date) 

# Diagnostics check
cat("Check: dates range =", format(min(dates_C, na.rm=TRUE)), "to", format(max(dates_C, na.rm=TRUE)), "\n")
cat("Check: pre idx count =", length(idx_pre_all), "post idx count =", length(idx_post_all), "\n")

# Helper Function: Compute Relative MSFE on a Sub-sample
# The function only uses rows where BOTH Model 1 (AR) and Model 2 (DI) have non-NA MSE values.
rel_msfe_subsample <- function(mse_mat, idx) {
    # Filter indices to include only rows where both models have a valid MSE
    ok <- idx[!is.na(mse_mat[idx, 1]) & !is.na(mse_mat[idx, 2])]
    
    if (length(ok) == 0) return(NA_real_) # Return NA if no usable data points
    
    # Calculate MSFE for the AR(6) model (Column 1)
    msfe_ar <- mean(mse_mat[ok, 1])
    # Calculate MSFE for the DI model (Column 2)
    msfe_di <- mean(mse_mat[ok, 2])
    
    # Return Relative MSFE (DI / AR)
    msfe_di / msfe_ar
}

# Apply the helper function to calculate relative MSFE for each sub-sample and horizon
stab_table <- data.frame(
    h = c(6, 12, 24),
    Rel_Pre2010 = c(rel_msfe_subsample(mse.h6, idx_pre_all),
                     rel_msfe_subsample(mse.h12, idx_pre_all),
                     rel_msfe_subsample(mse.h24, idx_pre_all)),
    Rel_Post2010 = c(rel_msfe_subsample(mse.h6, idx_post_all),
                     rel_msfe_subsample(mse.h12, idx_post_all),
                     rel_msfe_subsample(mse.h24, idx_post_all))
)

print(stab_table)

# Show counts of usable observations
# Provides transparency on the stability of the computed means.
ok_pre_24 <- idx_pre_all[!is.na(mse.h24[idx_pre_all,1]) & !is.na(mse.h24[idx_pre_all,2])]
ok_post_24 <- idx_post_all[!is.na(mse.h24[idx_post_all,1]) & !is.na(mse.h24[idx_post_all,2])]
cat("Usable rows for h=24: pre =", length(ok_pre_24), ", post =", length(ok_post_24), "\n")

```

Subsample Stability Analysis

To further assess the robustness of the diffusion index (DI) model, a subsample stability analysis is conducted by splitting the out-of-sample forecast period into pre-2010 and post-2010 subperiods. The analysis focuses on the four-factor DI specification and compares its performance with the AR(6) benchmark.

The results reveal pronounced time variation in forecasting performance. In the pre-2010 subsample, the diffusion index model performs favorably relative to the autoregressive benchmark at short and medium horizons. At forecast horizons of 6 and 12 months, the DI model yields lower mean squared forecast errors, indicating that prior to 2010 the estimated common factors contained predictive information not fully captured by the autoregressive dynamics of the target variable. At the longest horizon of 24 months, forecast performance is broadly comparable across the two models.

In sharp contrast, the diffusion index model performs substantially worse than the AR(6) benchmark in the post-2010 period. Relative forecast errors rise well above unity across all horizons, with particularly large deteriorations at short and medium horizons. This reversal indicates a breakdown in the predictive relationship between the estimated factors and the target variable in the later sample, highlighting a lack of temporal stability in the factor-based forecasting framework.

Several mechanisms may account for this instability. Structural changes in the macroeconomic environment following the global financial crisis, shifts in policy regimes, or changes in shock transmission mechanisms may have weakened the link between common macroeconomic factors and the target series. In addition, reduced signal strength or increased noise in the post-2010 period may have amplified estimation error and overfitting in the factor-augmented model, particularly within a rolling-window forecasting setting.

Overall, the subsample analysis demonstrates that the forecasting performance of diffusion index models is highly time-dependent. While factor-based models can deliver meaningful gains in certain periods, their predictive advantages are not stable over time. These findings underscore the importance of evaluating forecasting models across different subsamples and caution against drawing conclusions based solely on full-sample results.



## Conclusion

Overall, the analysis shows that the forecasting performance of diffusion index models is highly sensitive to both factor dimensionality and time variation. Although the information criteria proposed by Bai and Ng (2002) favor relatively large factor specifications, evidence from the scree plot suggests that the dominant common variation in the data is captured by a small number of factors.

Out-of-sample forecasting results indicate that increasing the number of factors beyond the four-factor baseline does not improve predictive performance and, in fact, leads to a deterioration in forecast accuracy, consistent with overfitting and increased estimation uncertainty. Moreover, the subsample analysis reveals pronounced time variation in performance, with the diffusion index model performing favorably prior to 2010 but deteriorating sharply thereafter.

Taken together, the findings highlight the importance of aligning factor selection with forecasting objectives and underscore that factor-based models are not only sensitive to model dimensionality but also to instability in predictive relationships over time. These results reinforce the role of parsimony and robustness considerations when implementing diffusion index forecasting models in practice.



Project Conclusion

This project examines the empirical performance of diffusion index forecasting models within a large macroeconomic dataset, with particular attention to factor estimation, factor dimensionality, and forecast robustness. Building on the static factor model framework of Stock and Watson (2002) and Bai and Ng (2002, 2008), the analysis evaluates whether augmenting a standard autoregressive benchmark with estimated common factors improves out-of-sample forecast accuracy for the target variable.

The analysis first establishes the theoretical foundation of the static factor model, emphasizing how a small number of latent factors can summarize the co-movement across a high-dimensional macroeconomic panel. Using principal components methods, the extracted factors exhibit substantial explanatory power for many individual series, supporting the relevance of a low-dimensional common component as a descriptive representation of the data.

The analysis then evaluates out-of-sample forecasting performance by comparing an AR(6) benchmark with diffusion index specifications at horizons h=6,12,24. Forecast accuracy is assessed using mean squared forecast errors (MSFE), consistent with a quadratic loss function that is standard in the diffusion index literature. Over the full out-of-sample period, the factor-augmented models do not outperform the autoregressive benchmark, indicating that—conditional on the target’s own lag dynamics—the estimated factors provide limited incremental predictive content in this application. The results are consistent with a trade-off highlighted in Bai and Ng’s framework: while factors can summarize broad information, adding many regressors can increase estimation uncertainty and weaken out-of-sample performance when the marginal predictive signal is small.

The analysis further investigates whether these conclusions are sensitive to factor dimensionality and to temporal instability. Although Bai and Ng information criteria favor relatively large numbers of factors, the scree plot suggests that the dominant common variation is concentrated in the first few components. Consistent with this diagnostic, expanding the factor dimension does not yield forecast gains and can worsen performance, consistent with overfitting. Moreover, subsample analysis reveals pronounced time variation: diffusion index forecasts perform relatively well prior to 2010 but deteriorate sharply thereafter, indicating that instability in the factor–target relationship is an important limitation for factor-based forecasting in this setting.

Beyond the baseline evaluation, several extensions could be considered to further probe robustness without altering the central findings. First, alternative loss functions and accuracy measures (e.g., MAE or other robust criteria) could be used to assess whether model rankings depend on the choice of quadratic loss. Second, longer-horizon forecasts (e.g., h=36 or h=48) could be explored, since factor-based models are sometimes argued to be more informative at low frequencies; however, the lack of relative improvement at h=24 and the reduced effective sample size at longer horizons suggest limited scope for a reversal in this application. Third, regularization methods such as LASSO could provide a principled way to control dimensionality by shrinking uninformative lagged factors toward zero, offering a compromise between information extraction and parsimony. Finally, modest refinements to the autoregressive benchmark—such as data-driven lag selection or alternative window lengths—could be used as additional robustness checks. While these extensions lie beyond the scope of the present analysis, they clarify that the main conclusions are not driven by a single modelling choice but reflect a broader empirical pattern: for the target variable considered, forecast performance is strongly influenced by estimation uncertainty and by time variation in predictive relationships, and a parsimonious autoregressive specification provides the most reliable performance over the full evaluation period.





